# -*- coding: utf-8 -*-
"""allGoalsPractice.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12j5b8FDSXzl8Uhk6J-Y9qIcVYguvUSMw
"""

!pip3 install cairosvg
!pip install apriltag

"""<br><br>
### **Aruco Markers**
<br><br>
"""

import cv2
from google.colab.patches import cv2_imshow
import cairosvg

import argparse
import imutils
import sys
import cv2.aruco as aruco
import numpy as np

# this is to generate the code the code with the white background for the canvas

marker_id = 23
marker_size = 200
canvas_size = 600  # total size of the white background


aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_50)
marker = aruco.generateImageMarker(aruco_dict, marker_id, marker_size)

canvas = 255 * np.ones((canvas_size, canvas_size), dtype=np.uint8)


start_x = (canvas_size - marker_size) // 2
start_y = (canvas_size - marker_size) // 2

canvas[start_y:start_y+marker_size, start_x:start_x+marker_size] = marker


cv2.imwrite(f"aruco_marker_id{marker_id}_white_background.png", canvas)

rm -rf arucoImgs

# need the eedit the code above to this, so that it generates different aruco
# of different sizes on white canvas
# https://github.com/KhairulIzwan/ArUco-markers-with-OpenCV-and-Python/blob/main/Generating%20ArUco%20markers/opencv_generate_aruco.py

import os
import random


canvas_size = 600
marker_size = 300

# define names of each possible ArUco tag OpenCV supports
arucoDict = {
	"DICT_4X4_50": cv2.aruco.DICT_4X4_50,
	"DICT_4X4_100": cv2.aruco.DICT_4X4_100,
	"DICT_4X4_250": cv2.aruco.DICT_4X4_250,
	"DICT_4X4_1000": cv2.aruco.DICT_4X4_1000,
	"DICT_5X5_50": cv2.aruco.DICT_5X5_50,
	"DICT_5X5_100": cv2.aruco.DICT_5X5_100,
	"DICT_5X5_250": cv2.aruco.DICT_5X5_250,
	"DICT_5X5_1000": cv2.aruco.DICT_5X5_1000,
	"DICT_6X6_50": cv2.aruco.DICT_6X6_50,
	"DICT_6X6_100": cv2.aruco.DICT_6X6_100,
	"DICT_6X6_250": cv2.aruco.DICT_6X6_250,
	"DICT_6X6_1000": cv2.aruco.DICT_6X6_1000,
	"DICT_7X7_50": cv2.aruco.DICT_7X7_50,
	"DICT_7X7_100": cv2.aruco.DICT_7X7_100,
	"DICT_7X7_250": cv2.aruco.DICT_7X7_250,
	"DICT_7X7_1000": cv2.aruco.DICT_7X7_1000,
	"DICT_ARUCO_ORIGINAL": cv2.aruco.DICT_ARUCO_ORIGINAL,
}




for dict_name, typeDict in arucoDict.items():
	aruco_dict = aruco.getPredefinedDictionary(typeDict)

	for i in range(1): # generate for each in the dictionary
		id = random.randint(1, 20)
		img = aruco.generateImageMarker(aruco_dict, id, marker_size)

		# to make a white background for the aruco marker
		canvas = 255 * np.ones((canvas_size, canvas_size), dtype=np.uint8)

		start_x = (canvas_size - marker_size) // 2
		start_y = (canvas_size - marker_size) // 2

		# Place the marker on the canvas
		canvas[start_y:start_y+marker_size, start_x:start_x+marker_size] = img

		cv2.imwrite(f"arucoImgs/aruco_marker_{id}.png", canvas)

print("Markers generated.")

#got code from here: https://github.com/KhairulIzwan/ArUco-markers-with-OpenCV-and-Python/blob/main/Detecting%20ArUco%20markers/detect_aruco_image.py


# define names of each possible ArUco tag OpenCV supports
ARUCO_DICT = {
	"DICT_4X4_50": cv2.aruco.DICT_4X4_50,
	"DICT_4X4_100": cv2.aruco.DICT_4X4_100,
	"DICT_4X4_250": cv2.aruco.DICT_4X4_250,
	"DICT_4X4_1000": cv2.aruco.DICT_4X4_1000,
	"DICT_5X5_50": cv2.aruco.DICT_5X5_50,
	"DICT_5X5_100": cv2.aruco.DICT_5X5_100,
	"DICT_5X5_250": cv2.aruco.DICT_5X5_250,
	"DICT_5X5_1000": cv2.aruco.DICT_5X5_1000,
	"DICT_6X6_50": cv2.aruco.DICT_6X6_50,
	"DICT_6X6_100": cv2.aruco.DICT_6X6_100,
	"DICT_6X6_250": cv2.aruco.DICT_6X6_250,
	"DICT_6X6_1000": cv2.aruco.DICT_6X6_1000,
	"DICT_7X7_50": cv2.aruco.DICT_7X7_50,
	"DICT_7X7_100": cv2.aruco.DICT_7X7_100,
	"DICT_7X7_250": cv2.aruco.DICT_7X7_250,
	"DICT_7X7_1000": cv2.aruco.DICT_7X7_1000,
	"DICT_ARUCO_ORIGINAL": cv2.aruco.DICT_ARUCO_ORIGINAL,
#	"DICT_APRILTAG_16h5": cv2.aruco.DICT_APRILTAG_16h5,
#	"DICT_APRILTAG_25h9": cv2.aruco.DICT_APRILTAG_25h9,
#	"DICT_APRILTAG_36h10": cv2.aruco.DICT_APRILTAG_36h10,
#	"DICT_APRILTAG_36h11": cv2.aruco.DICT_APRILTAG_36h11
}

print("[INFO] loading image...")
image = cv2.imread("/content/arucoImgs/aruco_marker_20.png")
image = imutils.resize(image, width=600)

gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# load the ArUCo dictionary, grab the ArUCo parameters, and detect

for (arucoName, arucoDict) in ARUCO_DICT.items():
  arucoDict = cv2.aruco.getPredefinedDictionary(arucoDict)
  arucoParams = cv2.aruco.DetectorParameters()


  detector = aruco.ArucoDetector(arucoDict, arucoParams)

  # Detect markers
  corners, ids, rejected = detector.detectMarkers(gray)

  # verify *at least* one ArUco marker was detected
  if len(corners) > 0:
    # flatten the ArUco IDs list
    ids = ids.flatten()

    # loop over the detected ArUCo corners
    for (markerCorner, markerID) in zip(corners, ids):
      # extract the marker corners (which are always returned in
      # top-left, top-right, bottom-right, and bottom-left order)
      corners = markerCorner.reshape((4, 2))
      (topLeft, topRight, bottomRight, bottomLeft) = corners

      # convert each of the (x, y)-coordinate pairs to integers
      topRight = (int(topRight[0]), int(topRight[1]))
      bottomRight = (int(bottomRight[0]), int(bottomRight[1]))
      bottomLeft = (int(bottomLeft[0]), int(bottomLeft[1]))
      topLeft = (int(topLeft[0]), int(topLeft[1]))

      # draw the bounding box of the ArUCo detection
      cv2.line(image, topLeft, topRight, (0, 255, 0), 2)
      cv2.line(image, topRight, bottomRight, (0, 255, 0), 2)
      cv2.line(image, bottomRight, bottomLeft, (0, 255, 0), 2)
      cv2.line(image, bottomLeft, topLeft, (0, 255, 0), 2)

      # compute and draw the center (x, y)-coordinates of the ArUco
      # marker
      cX = int((topLeft[0] + bottomRight[0]) / 2.0)
      cY = int((topLeft[1] + bottomRight[1]) / 2.0)
      cv2.circle(image, (cX, cY), 4, (0, 0, 255), -1)

      # draw the ArUco marker ID on the image
      cv2.putText(image, str(markerID),
        (topLeft[0], topLeft[1] - 15), cv2.FONT_HERSHEY_SIMPLEX,
        0.5, (0, 255, 0), 2)
      print("[INFO] ArUco marker ID: {}".format(markerID))

# show the output image
cv2_imshow(image)

"""
<br><br>
### **3D Reconstruction**
<br><br>"""

!pip install pyrealsense2 open3d

pip install torch torchvision torchaudio
pip install transformers

import matplotlib
from matplotlib import pyplot as plt
from PIL import Image
import torch
from transformers import GLPNImageProcessor, GLPNForDepthEstimation
import numpy as np
import open3d as o3d

feature_extractor = GLPNImageProcessor.from_pretrained("vinvino02/glpn-nyu")
model = GLPNForDepthEstimation.from_pretrained("vinvino02/glpn-nyu")

image = Image.open('sam.png')
new_height = 480 if image.height > 480 else image.height
new_height -= (new_height % 32)
new_width = int(new_height * image.width / image.height)
diff = new_width % 32
new_width = new_width - diff if diff < 16 else new_width + 32 - diff
new_size = (new_height, new_width)
image = image.resize(new_size)

image_rgb = image.convert("RGB")
image_np = np.array(image_rgb).astype(np.float32)

inputs = feature_extractor(images=image_np, return_tensors='pt')
with torch.no_grad():
    outputs = model(**inputs)
    predicted_depth = outputs.predicted_depth

from PIL import Image # Make sure Image is imported
import numpy as np # Make sure numpy is imported

image = Image.open('sam.png')
# Store the original image size if needed later, or just use image.size
original_width, original_height = image.size

new_height = 480 if image.height > 480 else image.height
new_height -= (new_height % 32)
new_width = int(new_height * image.width / image.height)
diff = new_width % 32
new_width = new_width - diff if diff < 16 else new_width + 32 - diff
new_size = (new_width, new_height) # PIL resize expects (width, height)
image_resized_pil = image.resize(new_size)

# Keep the original 'image' variable referencing the original PIL image if you need it later.
# Use 'image_resized_pil' for the input to the depth model.

# file ipython-input-12-4c059bf5d90f
pad = 16
# Crop the depth map (output is already a NumPy array)
output = predicted_depth.squeeze().cpu().numpy() * 1000.0
output = output[pad:-pad, pad:-pad]

# 'image' at this point should still be the resized PIL Image object from ipython-input-3
# or the original PIL image if ipython-input-3 didn't reassign 'image'.
# Let's assume 'image_resized_pil' is the variable holding the resized PIL image.
# Crop the PIL Image object using its built-in method.
# The crop box is a tuple (left, upper, right, lower).
resized_width, resized_height = image_resized_pil.size
# Crop the resized PIL image
image_cropped_pil = image_resized_pil.crop((pad, pad, resized_width - pad, resized_height - pad))

# Convert the cropped PIL image to a NumPy array for plotting and point cloud creation
image_np_cropped = np.array(image_cropped_pil)

# Use the cropped NumPy array for plotting
fig, ax = plt.subplots(1, 2)
# Use the cropped NumPy array for imshow
ax[0].imshow(image_np_cropped)
ax[0].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)
ax[1].imshow(output, cmap='plasma')
ax[1].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)
plt.tight_layout()
plt.pause(5)

fig, ax = plt.subplots(1, 2)
ax[0].imshow(image)
ax[0].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)
ax[1].imshow(output, cmap='plasma')
ax[1].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)
plt.tight_layout()
plt.pause(5)

def create_point_cloud_from_depth_image(depth_image, intrinsic, scale=1.0):
    """
    Create a point cloud from a depth image using Open3D.
    Parameters:
        - depth_image: A numpy array containing the depth image.
        - intrinsic: An Open3D camera intrinsic object.
        - scale: A scaling factor to adjust the depth values.
    Returns:
        - A point cloud object.
        """

    height, width = depth_image.shape
    depth_image = o3d.geometry.Image((depth_image / scale).astype(np.float32))    # Create an RGBD image
    rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(
    color=o3d.geometry.Image(np.zeros((height, width, 3), dtype=np.uint8)),
    depth=depth_image,convert_rgb_to_intensity=False)

    # Create the point cloud from the RGBD image
    pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd_image, intrinsic)
    return pcd

fx = 300  # Focal length in the x axis
fy = 300  # Focal length in the y axis
cx = output.shape[1] / 2  # Principal point in the x axis
cy = output.shape[0] / 2  # Principal point in the y axis
intrinsic = o3d.camera.PinholeCameraIntrinsic(
    width=output.shape[1],
    height=output.shape[0],
    fx=fx,
    fy=fy,
    cx=cx,
    cy=cy
)

# Create the point cloud from the depth image
pcd = create_point_cloud_from_depth_image(output, intrinsic, scale=1)
# Visualize the point cloud
o3d.visualization.draw_plotly([pcd])

o3d.io.write_point_cloud("output_point_cloud.ply", pcd)

pcd_loaded = o3d.io.read_point_cloud("output_point_cloud.ply")
point_cloud = np.asarray(pcd_loaded.points)

# had to change it over here - check the difference, something to do with how the cloud was saved
pcd = o3d.geometry.PointCloud()
pcd.points = o3d.utility.Vector3dVector(point_cloud[:,:3])
colors_data = np.asarray(pcd.colors)
pcd_normal = np.asarray(pcd.normals)

o3d.visualization.draw_plotly([pcd])

# had to change it over here - check the difference, something to do with how the cloud was saved
pcd = o3d.geometry.PointCloud()
pcd.points = o3d.utility.Vector3dVector(point_cloud[:,:3])

# Estimate normals for the point cloud
# Search radius for KNN
pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamKNN(knn=30))

# Optionally, orient the normals consistently
pcd.orient_normals_consistent_tangent_plane(k=30)


colors_data = np.asarray(pcd.colors) # This will be empty unless you add colors to the point cloud
pcd_normal = np.asarray(pcd.normals) # Now this will contain the estimated normals

# Check if normals were estimated
if not pcd.has_normals():
    print("Error: Normals were not estimated correctly.")
else:
    print("Normals estimated successfully.")


o3d.visualization.draw_plotly([pcd])


distances = pcd.compute_nearest_neighbor_distance()
avg_dist = np.mean(distances)
radius = 3 * avg_dist

# Now that the point cloud has normals, ball pivoting should work
bpa_mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_ball_pivoting(pcd, o3d.utility.DoubleVector([radius, radius * 2]))

# You need to define or load the 'mesh' variable if you intend to use it here.
# Based on the context, it seems you intended to simplify the bpa_mesh.
# Replace 'mesh' with 'bpa_mesh'
dec_mesh = bpa_mesh.simplify_quadric_decimation(100000)

dec_mesh.remove_degenerate_triangles()
dec_mesh.remove_duplicated_triangles()
dec_mesh.remove_duplicated_vertices()
dec_mesh.remove_non_manifold_edges()

# Visualize the resulting mesh
o3d.visualization.draw_plotly([dec_mesh])

o3d.io.write_triangle_mesh("bpa_mesh.ply", dec_mesh)

import cv2
from PIL import Image
import numpy as np
import matplotlib
from matplotlib import pyplot as plt

"""<br><br>
### **Room Mapping**
<br><br>
"""

image_pil = Image.open('sam.png')
# Convert PIL image to NumPy array for OpenCV
image = cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR) #

# Initialize the ORB detector with 5k keypoints
orb = cv2.ORB_create(nfeatures=5000)

# Detect keypoints and compute descriptors
keypoints1, descriptors1 = orb.detectAndCompute(image, None)

# Detect keypoints and compute descriptors for the second image
keypoints2, descriptors2 = orb.detectAndCompute(image, None)

# Initialize the BFMatcher with default params
bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)

# Match descriptors between frame I and frame I+1
matches = bf.match(descriptors1, descriptors2)

# Sort matches by distance (best matches first)
matches = sorted(matches, key=lambda x: x.distance)

image1 = cv2.imread('sam.png', cv2.IMREAD_GRAYSCALE)

# Load the subsequent image (frame I+1)
image2 = cv2.imread('sam.png', cv2.IMREAD_GRAYSCALE)

# Initialize the ORB detector
orb = cv2.ORB_create(nfeatures=10000)

# Detect keypoints and compute descriptors for the first image
keypoints1, descriptors1 = orb.detectAndCompute(image1, None)

# Detect keypoints and compute descriptors for the second image
keypoints2, descriptors2 = orb.detectAndCompute(image2, None)

# Initialize the BFMatcher with default params
bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)

# Match descriptors
matches = bf.match(descriptors1, descriptors2)

# Sort matches by distance (best matches first)
matches = sorted(matches, key=lambda x: x.distance)

# Draw the top N matches (for example, the top 50 matches)
top_matches = matches[:50]
matched_image = cv2.drawMatches(image1, keypoints1, image2, keypoints2, top_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)

# Display the image with matches
plt.figure(figsize=(20, 15))
plt.imshow(matched_image)
plt.title('ORB Feature Matches between Frame I and Frame I+1')
plt.show()

# Optionally, extract the matched points
matched_points1 = np.float32([keypoints1[m.queryIdx].pt for m in matches])
matched_points2 = np.float32([keypoints2[m.trainIdx].pt for m in matches])

# Print the coordinates of the matched points
print("Matched points in frame I:")
print(matched_points1)
print("Matched points in frame I+1:")
print(matched_points2)

# pose estimation
K = np.array([[3177.000, 0, 1632.000],
            [0, 3177.00, 1224.00],
            [0, 0, 1]])


# Optionally, extract the matched points
pts1 = np.float32([keypoints1[m.queryIdx].pt for m in matches])
pts2 = np.float32([keypoints2[m.trainIdx].pt for m in matches])

# Compute the Fundamental matrix using RANSAC
F, inliers = cv2.findFundamentalMat(pts1, pts2, cv2.FM_RANSAC)

# Select inlier points
pts1_inliers = pts1[inliers.ravel() == 1]
pts2_inliers = pts2[inliers.ravel() == 1]

# estimate essential matrix
E = K.T @ F @ K

_, R, t, mask = cv2.recoverPose(E, pts1_inliers, pts2_inliers, K)

print("Rotation matrix R:")
print(R)
print("Translation vector t:")
print(t)

def add_ones(x):
    return np.concatenate([x, np.ones((x.shape[0], 1))], axis=1)

T = np.eye(4)
T[0:3,0:3] = R
T[0:3,3] = t.T
Kinv = np.linalg.inv(K)
pose1 = np.eye(4)
pose2 = np.eye(4) @ T

# Optionally, extract the matched points
pts1 = np.float32([keypoints1[m.queryIdx].pt for m in matches])
pts2 = np.float32([keypoints2[m.trainIdx].pt for m in matches])



ret = np.zeros((pts1.shape[0], 4))
pose1 = np.linalg.inv(pose1)
pose2 = np.linalg.inv(pose2)
for i, p in enumerate(zip(add_ones(pts1),
                          add_ones(pts2))):
    A = np.zeros((4, 4))
    A[0] = p[0][0] * pose1[2] - pose1[0]
    A[1] = p[0][1] * pose1[2] - pose1[1]
    A[2] = p[1][0] * pose2[2] - pose2[0]
    A[3] = p[1][1] * pose2[2] - pose2[1]
    _, _, vt = np.linalg.svd(A)
    ret[i] = vt[3]

ret /= ret[:, 3:]
good_pts4d =   (np.abs(ret[:, 3]) > 0.005) & (ret[:, 2] > 0)

mapp_pts = [p for i, p in enumerate(ret) if good_pts4d[i]]

import open3d as o3d
import numpy as np

# Assume mapp_pts is a list of 3D points obtained from the previous code
mapp_pts = np.array(mapp_pts)  # Convert to a NumPy array if it's not already

# Create a PointCloud object
pcd = o3d.geometry.PointCloud()

# Convert the NumPy array to Open3D format
pcd.points = o3d.utility.Vector3dVector(mapp_pts[:, :3])

# Create an array of white colors
colors = np.zeros((mapp_pts.shape[0], 3))
colors[:, 1] = 1  # Set the green channel to 1

# Assign the colors to the point cloud
pcd.colors = o3d.utility.Vector3dVector(colors)
o3d.visualization.draw_plotly([pcd])
# Save the point cloud to a .pcd file
o3d.io.write_point_cloud("points_colored_structurev2.ply", pcd)